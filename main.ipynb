{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST Fashion Dataset: A ConvNet Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "import gzip\r\n",
    "import numpy as np\r\n",
    "from functools import reduce\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from typing import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Activation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Applies the logistic sigmoid function to the given values\"\"\"\r\n",
    "    return 1 / (1 + np.exp(-z))\r\n",
    "\r\n",
    "def sigmoid_g(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Computes the gradients of the sigmoid function\"\"\"\r\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\r\n",
    "\r\n",
    "def tanh(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Applies the TanH activation function to the given values\"\"\"\r\n",
    "    return np.tanh(z)\r\n",
    "\r\n",
    "def tanh_g(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Computes the gradients of the TanH function\"\"\"\r\n",
    "    return 1 - tanh(z) ** 2\r\n",
    "\r\n",
    "def relu(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Applies the ReLU activation function to the given values\"\"\"\r\n",
    "    return z.clip(0)\r\n",
    "\r\n",
    "def relu_g(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Computes the gradients of the ReLU function\"\"\"\r\n",
    "    r: np.ndarray = np.zeros(z.shape)\r\n",
    "    r[z>0] = 1\r\n",
    "    return r"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weight Initialization Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "def uniform_init(inp: int, out: int, shape: List[int]) -> np.ndarray:\r\n",
    "    \"\"\"\r\n",
    "    Generates initalized weights of a given shape\r\n",
    "    :param inp: The number of inputs\r\n",
    "    :param out: The number of outputs\r\n",
    "    :param shape: The shape of the weights to be initialized\r\n",
    "    :return: The uniformly randomly initalized weights\r\n",
    "    \"\"\"\r\n",
    "    return np.random.uniform(-1, 1, shape)\r\n",
    "\r\n",
    "def standard_init(inp: int, out: int, shape: List[int]) -> np.ndarray:\r\n",
    "    \"\"\"\r\n",
    "    Generates initalized weights of a given shape\r\n",
    "    :param inp: The number of inputs\r\n",
    "    :param out: The number of outputs\r\n",
    "    :param shape: The shape of the weights to be initialized\r\n",
    "    :return: The \"Standard\"-initalized weights\r\n",
    "    \"\"\"\r\n",
    "    return np.random.uniform(-1, 1, shape) * np.sqrt(1 / inp)\r\n",
    "\r\n",
    "def xavier_init(inp: int, out: int, shape: List[int]) -> np.ndarray:\r\n",
    "    \"\"\"\r\n",
    "    Generates initalized weights of a given shape\r\n",
    "    :param inp: The number of inputs\r\n",
    "    :param out: The number of outputs\r\n",
    "    :param shape: The shape of the weights to be initialized\r\n",
    "    :return: The \"Xavier\"-initalized weights\r\n",
    "    \"\"\"\r\n",
    "    return np.random.uniform(-1, 1, shape) * np.sqrt(6 / (inp + out))\r\n",
    "\r\n",
    "def he_init(inp: int, out: int, shape: List[int]) -> np.ndarray:\r\n",
    "    \"\"\"\r\n",
    "    Generates initalized weights of a given shape\r\n",
    "    :param inp: The number of inputs\r\n",
    "    :param out: The number of outputs\r\n",
    "    :param shape: The shape of the weights to be initialized\r\n",
    "    :return: The \"He\"-initalized weights\r\n",
    "    \"\"\"\r\n",
    "    return np.random.uniform(-1, 1, shape) * np.sqrt(2 / inp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Layer Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "class Layer(object):\r\n",
    "    \"\"\"\r\n",
    "    Represents a layer in a deep neural network.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, actf: Callable[[np.ndarray], np.ndarray] = relu, actf_g: Callable[[np.ndarray], np.ndarray] = relu_g,\r\n",
    "                 initializer: Callable[[int, int, List[int]], np.ndarray] = he_init):\r\n",
    "        \"\"\"\r\n",
    "        Create a new layer.\r\n",
    "        :param actf: The activation function to use\r\n",
    "        :param actf_g: The activation function's derivation\r\n",
    "        :param initializer: The weight initializer to use\r\n",
    "        :param inp_shape: The shape of the layer's input\r\n",
    "        \"\"\"\r\n",
    "        self.actf: Callable[[np.ndarray], np.ndarray] = actf\r\n",
    "        self.actf_g: Callable[[np.ndarray], np.ndarray] = actf_g\r\n",
    "        self.initializer: Callable[[int, int, List[int]], np.ndarray] = initializer\r\n",
    "        self.weights: Optional[np.ndarray] = None\r\n",
    "\r\n",
    "    def initialize(self) -> None:\r\n",
    "        \"\"\"Initializes the weights\"\"\"\r\n",
    "        raise NotImplementedError()\r\n",
    "\r\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        raise NotImplementedError()\r\n",
    "    \r\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform backward propagation + return the results.\"\"\"\r\n",
    "        raise NotImplementedError()\r\n",
    "    \r\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        return self.forward(inp) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "class FCLayer(Layer):\r\n",
    "    \"\"\"\r\n",
    "    Represents a fully-connected layer in a deep neural network.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, neurons: int, actf: Callable[[np.ndarray], np.ndarray] = relu, actf_g: Callable[[np.ndarray], np.ndarray] = relu_g,\r\n",
    "                 initializer: Callable[[int, int, List[int]], np.ndarray] = he_init, inp_shape: Optional[Tuple[int]] = None):\r\n",
    "        \"\"\"\r\n",
    "        Create a new fully-connected layer.\r\n",
    "        :param neurons: The number of neurons in the layer\r\n",
    "        :param actf: The activation function to use\r\n",
    "        :param actf_g: The activation function's derivation\r\n",
    "        :param initializer: The weight initializer to use\r\n",
    "        :param inp_shape: The shape of the layer's input - format (#neurons)\r\n",
    "        \"\"\"\r\n",
    "        self.neurons: int = neurons\r\n",
    "        super().__init__(actf, actf_g, initializer)\r\n",
    "        self.inp_shape: Optional[Tuple[int]] = inp_shape\r\n",
    "\r\n",
    "    @property\r\n",
    "    def out_shape(self) -> Tuple[int]:\r\n",
    "        return ( self.neurons, )\r\n",
    "\r\n",
    "    def initialize(self) -> None:\r\n",
    "        \"\"\"Initializes the weights\"\"\"\r\n",
    "        if self.inp_shape is None:\r\n",
    "            return\r\n",
    "        self.weights = self.initializer(self.inp_shape[0] + 1, self.neurons, [ self.inp_shape[0] + 1, self.neurons, ])\r\n",
    "\r\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        return self.actf(self._forward(inp))\r\n",
    "\r\n",
    "    def _forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation - without applying the activation function.\"\"\"\r\n",
    "        assert len(inp.shape) == 2\r\n",
    "        assert inp.shape[0] == self.inp_shape[0]\r\n",
    "        return self.weights.T @ np.vstack((inp, np.ones(inp.shape[1])))\r\n",
    "    \r\n",
    "    def backward(self, grad: np.ndarray, val: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"\r\n",
    "        Perform backward propagation + return the results.\r\n",
    "        :param grad: The gradient of the next layer\r\n",
    "        :param val: This layer's value (= before application of activation function)\r\n",
    "        :return: This layer's gradient\r\n",
    "        \"\"\"\r\n",
    "        assert len(grad.shape) == 2\r\n",
    "        assert grad.shape == val.shape\r\n",
    "        assert grad.shape[0] == self.neurons\r\n",
    "        return self.weights[:-1, :] @ (grad * self.actf_g(val))\r\n",
    "\r\n",
    "    def _backward(self, grad: np.ndarray, val: np.ndarray, p_act: np.ndarray, lam: float) -> np.ndarray:\r\n",
    "        \"\"\"\r\n",
    "        Perform backward propagation - but return the weight's gradients.\r\n",
    "        :param grad: The gradient of the next layer\r\n",
    "        :param val: This layer's value (= before application of activation function)\r\n",
    "        :param p_act: The previous layer's activation\r\n",
    "        :param lam: The regularization parameter lambda - for weight regularization\r\n",
    "        :return: This layer's gradient\r\n",
    "        \"\"\"\r\n",
    "        assert len(grad.shape) == 2\r\n",
    "        assert grad.shape == val.shape\r\n",
    "        assert grad.shape[0] == self.neurons\r\n",
    "        assert p_act.shape[1] == val.shape[1]\r\n",
    "        assert p_act.shape[0] == self.inp_shape[0]\r\n",
    "        return np.vstack((p_act, np.ones(p_act.shape[1]))) @ (grad * self.actf_g(val)).T + (lam/val.shape[1]) * self.weights\r\n",
    "\r\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        return self.forward(inp) \r\n",
    "\r\n",
    "    def __setattr__(self, name: str, value: Any) -> None:\r\n",
    "        if name == 'inp_shape' and value and len(value) != 1:\r\n",
    "            raise TypeError()\r\n",
    "        elif name in ('neurons', 'inp_shape'):\r\n",
    "            self.__dict__[name] = value\r\n",
    "            if 'neurons' in self.__dict__.keys() and 'inp_shape' in self.__dict__.keys():\r\n",
    "                self.initialize()\r\n",
    "            return\r\n",
    "        elif name == 'initializer':\r\n",
    "            self.__dict__[name] = value\r\n",
    "            if 'shape' in self.__dict__.keys() and 'inp_shape' in self.__dict__.keys():\r\n",
    "                self.initialize()\r\n",
    "            return\r\n",
    "        super().__setattr__(name, value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "class ConvLayer(Layer):\r\n",
    "    \"\"\"\r\n",
    "    Represents a convolutional layer in a deep neural network.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, shape: Tuple[int, int, int], stride: int, pad: int = 0, \r\n",
    "                 actf: Callable[[np.ndarray], np.ndarray] = relu, actf_g: Callable[[np.ndarray], np.ndarray] = relu_g,\r\n",
    "                 initializer: Callable[[int, int, List[int]], np.ndarray] = he_init, inp_shape: Optional[Tuple[int, int, int]] = None):\r\n",
    "        \"\"\"\r\n",
    "        Create a new convolutional layer.\r\n",
    "        :param shape: The shape of the layer's filters - format (#filters x height x width)\r\n",
    "        :param stride: The filters' stride\r\n",
    "        :param actf: The activation function to use\r\n",
    "        :param actf_g: The activation function's derivation\r\n",
    "        :param initializer: The weight initializer to use\r\n",
    "        :param inp_shape: The shape of the layer's input - format (height x width x #channels)\r\n",
    "        \"\"\"\r\n",
    "        self.shape: List[int] = shape\r\n",
    "        self.stride: int = stride\r\n",
    "        self.pad: int = pad\r\n",
    "        super().__init__(actf, actf_g, initializer)\r\n",
    "        self.inp_shape: Optional[Tuple[int, int, int]] = inp_shape\r\n",
    "\r\n",
    "    @classmethod\r\n",
    "    def _convolve(cls, x: np.ndarray, f: np.ndarray, p: int = 0, s: int = 1) -> np.ndarray:\r\n",
    "        \"\"\"\r\n",
    "        Perform convolution + return result.\r\n",
    "        :param x: The base array to convolve on - shape: ( #inputs, height, width, #channels )\r\n",
    "        :param f: The filter(s) to use - shape: ( #filters, height, width, #channels )\r\n",
    "        :param p: The padding (added on all sides) - default: 0\r\n",
    "        :param s: The stride - default: 0\r\n",
    "        \"\"\"\r\n",
    "        assert len(x.shape) == 4\r\n",
    "        padded: np.ndarray = np.pad(x, [(0, 0), *((p,)*2,)*2, (0, 0)])\r\n",
    "        strides: np.ndarray = np.lib.stride_tricks.sliding_window_view(padded, (x.shape[0], *f.shape[1:]))\r\n",
    "        strides = np.moveaxis(np.repeat(strides.reshape(*strides.shape[1:3], x.shape[0], 1, *strides.shape[5:7], f.shape[-1])[::s, ::s], \r\n",
    "                \t                    f.shape[0], axis=3) * f, 2, 0)\r\n",
    "        return np.sum(strides.reshape(*strides.shape[:4], -1), axis=4)\r\n",
    "\r\n",
    "    @classmethod\r\n",
    "    def _full_convolve(cls, x: np.ndarray, f: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"\r\n",
    "        Perform \"full\" convolution + return result.\r\n",
    "        :param x: The base array to convolve on - shape: ( #inputs, height, width, #channels )\r\n",
    "        :param f: The filter(s) to use - shape: ( #filters, height, width, #channels )\r\n",
    "        \"\"\"\r\n",
    "        return cls._convolve(x, f, f.shape[1] - 1)\r\n",
    "\r\n",
    "    @property\r\n",
    "    def out_shape(self) -> Tuple[int, int, int]:\r\n",
    "        return ( int(1 + (self.inp_shape[0] + 2 * self.pad - self.shape[1]) / self.stride),\r\n",
    "                 int(1 + (self.inp_shape[1] + 2 * self.pad - self.shape[2]) / self.stride),\r\n",
    "                 self.shape[0], )\r\n",
    "\r\n",
    "    def initialize(self) -> None:\r\n",
    "        \"\"\"Initializes the weights\"\"\"\r\n",
    "        if self.inp_shape is None:\r\n",
    "            raise TypeError()\r\n",
    "        filters_size: int = reduce(lambda x, y: x*y, self.shape)\r\n",
    "        self.weights = self.initializer(filters_size * self.inp_shape[2], filters_size, [ *self.shape, self.inp_shape[2], ])\r\n",
    "    \r\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        return self.actf(self._forward(inp))\r\n",
    "\r\n",
    "    def _forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation - without applying the activation function.\"\"\"\r\n",
    "        assert inp.shape[1:] == self.inp_shape\r\n",
    "        return self._convolve(inp, self.weights, self.pad, self.stride)\r\n",
    "\r\n",
    "    def backward(self, grad: np.ndarray, val: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"\r\n",
    "        Perform backward propagation + return the results.\r\n",
    "        :param grad: The gradient of the next layer\r\n",
    "        :param val: This layer's value (= before application of activation function)\r\n",
    "        :return: This layer's gradient\r\n",
    "        \"\"\"\r\n",
    "        assert grad.shape[1:] == self.out_shape\r\n",
    "        assert val.shape[1:] == self.out_shape\r\n",
    "        return self._full_convolve(grad * self.actf_g(val), np.swapaxes(np.flip(self.weights, axis=(1,2)), 0, 3))\r\n",
    "\r\n",
    "    def _backward(self, grad: np.ndarray, val: np.ndarray, p_act: np.ndarray, lam: float) -> np.ndarray:\r\n",
    "        \"\"\"\r\n",
    "        Perform backward propagation - but return the weight's gradients.\r\n",
    "        :param grad: The gradient of the next layer\r\n",
    "        :param val: This layer's value (= before application of activation function)\r\n",
    "        :param p_act: The previous layer's activation\r\n",
    "        :param lam: The regularization parameter lambda - for weight regularization\r\n",
    "        :return: This layer's gradient\r\n",
    "        \"\"\"\r\n",
    "        assert grad.shape[1:] == self.out_shape\r\n",
    "        assert val.shape[1:] == self.out_shape\r\n",
    "        return self._convolve(np.swapaxes(p_act, 0, 3), np.swapaxes(grad * self.actf_g(val), 0, 3)).swapaxes(0, 3) + (lam/val.shape[1]) * self.weights\r\n",
    "\r\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        return self.forward(inp) \r\n",
    "\r\n",
    "    def __setattr__(self, name: str, value: Any) -> None:\r\n",
    "        if name in ('shape', 'inp_shape',) and value:\r\n",
    "            if len(value) != 3:\r\n",
    "                raise TypeError()\r\n",
    "            else:\r\n",
    "                self.__dict__[name] = value\r\n",
    "                if 'shape' in self.__dict__.keys() and 'inp_shape' in self.__dict__.keys():\r\n",
    "                    self.initialize()\r\n",
    "                return\r\n",
    "        elif name == 'initializer':\r\n",
    "            self.__dict__[name] = value\r\n",
    "            if 'shape' in self.__dict__.keys() and 'inp_shape' in self.__dict__.keys():\r\n",
    "                self.initialize()\r\n",
    "            return\r\n",
    "        super().__setattr__(name, value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "class FlattenLayer(Layer):\r\n",
    "    \"\"\"\r\n",
    "    Flattens the output of a previous layer in a neural network.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, inp_shape: Optional[Tuple[int, int, int]] = None):\r\n",
    "        \"\"\"\r\n",
    "        Create a new flatten layer.\r\n",
    "        :param inp_shape: The shape of the layer's input - format (height x width x #channels)\r\n",
    "        \"\"\"\r\n",
    "        self.inp_shape: Optional[Tuple[int, int, int]] = inp_shape\r\n",
    "\r\n",
    "    @property\r\n",
    "    def out_shape(self) -> Tuple[int]:\r\n",
    "        return ( reduce(lambda a,b: a*b, self.inp_shape), )\r\n",
    "\r\n",
    "    def initialize(self) -> None:\r\n",
    "        \"\"\"Dummy function - layer has no weights\"\"\"\r\n",
    "        pass\r\n",
    "\r\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        return inp.reshape(-1, *self.out_shape).T\r\n",
    "\r\n",
    "    def backward(self, grad: np.ndarray, *args: List[Any]) -> np.ndarray:\r\n",
    "        \"\"\"\r\n",
    "        Perform backward propagation + return the results.\r\n",
    "        :param grad: The gradient of the next layer\r\n",
    "        :return: This layer's gradient (= the reshaped gradient)\r\n",
    "        \"\"\"\r\n",
    "        return grad.reshape(-1, *self.inp_shape).T\r\n",
    "\r\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        return self.forward(inp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "class Network(object):\r\n",
    "    \"\"\"\r\n",
    "    Represents a deep neural network.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, initializer: Callable[[List[int]], List[np.ndarray]] = he_init, layers: List[Layer] = []):\r\n",
    "        \"\"\"\r\n",
    "        Create a new deep neural network.\r\n",
    "        :param initializer: The weight initializer to use\r\n",
    "        :param layers: The network's layers\r\n",
    "        \"\"\"\r\n",
    "        self.__initializer: Callable[[List[int]], List[np.ndarray]] = initializer\r\n",
    "        self.layers: List[Layer] = layers\r\n",
    "\r\n",
    "    @property\r\n",
    "    def inp_shape(self) -> List[int]:\r\n",
    "        return self.layers[0].inp_shape if self.layers else None\r\n",
    "\r\n",
    "    @property\r\n",
    "    def out_shape(self) -> List[int]:\r\n",
    "        return self.layers[-1].out_shape if self.layers else None\r\n",
    "\r\n",
    "    def initialize(self) -> None:\r\n",
    "        \"\"\"Re-initialize all weights\"\"\"\r\n",
    "        for l in self.layers:\r\n",
    "            l.initialize()\r\n",
    "\r\n",
    "    def __prepare_layer(self, layer: Layer) -> None:\r\n",
    "        if self.layers:\r\n",
    "            layer.inp_shape = self.out_shape\r\n",
    "        layer.initializer = self.__initializer\r\n",
    "\r\n",
    "    def append(self, layer: Layer) -> None:\r\n",
    "        \"\"\"Appends a new layer to the network\"\"\"\r\n",
    "        self.__prepare_layer(layer)\r\n",
    "        self.layers.append(layer)\r\n",
    "        self.initialize()\r\n",
    "\r\n",
    "    def insert(self, idx: int, layer: Layer) -> None:\r\n",
    "        \"\"\"Inserts the given layer at the given index\"\"\"\r\n",
    "        self.__prepare_layer(layer)\r\n",
    "        self.layers.insert(idx, layer)\r\n",
    "        self.initialize()\r\n",
    "\r\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation using the given input\"\"\"\r\n",
    "        out: np.ndarray = x\r\n",
    "        for l in self.layers:\r\n",
    "            out = l(out)\r\n",
    "        return out\r\n",
    "\r\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        return self.forward(inp) \r\n",
    "\r\n",
    "    def __add__(self, other: Any) -> Any:\r\n",
    "        if not isinstance(other, Layer):\r\n",
    "            raise ArithmeticError()\r\n",
    "        self.__prepare_layer(other)\r\n",
    "        return Network(self.__initializer, [ *self.layers.copy(), other, ])\r\n",
    "\r\n",
    "    def __iadd__(self, other: Any) -> None:\r\n",
    "        if not isinstance(other, Layer):\r\n",
    "            raise ArithmeticError()\r\n",
    "        self.append(other)\r\n",
    "        return self\r\n",
    "\r\n",
    "    def __len__(self) -> int:\r\n",
    "        return len(self.layers)\r\n",
    "\r\n",
    "    def __getitem__(self, key: int) -> Layer:\r\n",
    "        return self.layers[key]\r\n",
    "\r\n",
    "    def __setitem__(self, key: int, value: Layer) -> None:\r\n",
    "        if key > 0:\r\n",
    "            value.inp_shape = self[key-1].out_shape\r\n",
    "        self.layers[key] = value\r\n",
    "        self.initialize()\r\n",
    "    \r\n",
    "    def __delitem__(self, key: int) -> None:\r\n",
    "        del self.layers[key]\r\n",
    "        self.initialize()\r\n",
    "\r\n",
    "    def __str__(self) -> str:\r\n",
    "        return 'DNN{ ' + ' --> '.join(f'{l.__class__.__name__}{l.out_shape}' for l in self.layers) + ' }'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "n: Network = Network(initializer=he_init)\r\n",
    "\r\n",
    "n += ConvLayer((8, 5, 5), 1, inp_shape=(28, 28, 3))\r\n",
    "n += ConvLayer((16, 3, 3), 1)\r\n",
    "n += FlattenLayer()\r\n",
    "n += FCLayer(150)\r\n",
    "n += FCLayer(10)\r\n",
    "\r\n",
    "print(n)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DNN{ ConvLayer(24, 24, 8) --> ConvLayer(22, 22, 16) --> FlattenLayer(7744,) --> FCLayer(150,) --> FCLayer(10,) }\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit"
  },
  "interpreter": {
   "hash": "2f4b728787ec198e2dbbb031677efb56b168098f9046d913c03fa1c4e7b35f03"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}