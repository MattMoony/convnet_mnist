{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST Fashion Dataset: A ConvNet Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import gzip\r\n",
    "import numpy as np\r\n",
    "from functools import reduce\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from typing import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Activation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Applies the logistic sigmoid function to the given values\"\"\"\r\n",
    "    return 1 / (1 + np.exp(-z))\r\n",
    "\r\n",
    "def sigmoid_g(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Computes the gradients of the sigmoid function\"\"\"\r\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\r\n",
    "\r\n",
    "def tanh(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Applies the TanH activation function to the given values\"\"\"\r\n",
    "    return np.tanh(z)\r\n",
    "\r\n",
    "def tanh_g(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Computes the gradients of the TanH function\"\"\"\r\n",
    "    return 1 - tanh(z) ** 2\r\n",
    "\r\n",
    "def relu(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Applies the ReLU activation function to the given values\"\"\"\r\n",
    "    return z.clip(0)\r\n",
    "\r\n",
    "def relu_g(z: np.ndarray) -> np.ndarray:\r\n",
    "    \"\"\"Computes the gradients of the ReLU function\"\"\"\r\n",
    "    r: np.ndarray = np.zeros(z.shape)\r\n",
    "    r[z>0] = 1\r\n",
    "    return r"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weight Initialization Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def uniform_init(inp: int, out: int, shape: List[int]) -> np.ndarray:\r\n",
    "    \"\"\"\r\n",
    "    Generates initalized weights of a given shape\r\n",
    "    :param inp: The number of inputs\r\n",
    "    :param out: The number of outputs\r\n",
    "    :param shape: The shape of the weights to be initialized\r\n",
    "    :return: The uniformly randomly initalized weights\r\n",
    "    \"\"\"\r\n",
    "    return np.random.uniform(-1, 1, shape)\r\n",
    "\r\n",
    "def standard_init(inp: int, out: int, shape: List[int]) -> np.ndarray:\r\n",
    "    \"\"\"\r\n",
    "    Generates initalized weights of a given shape\r\n",
    "    :param inp: The number of inputs\r\n",
    "    :param out: The number of outputs\r\n",
    "    :param shape: The shape of the weights to be initialized\r\n",
    "    :return: The \"Standard\"-initalized weights\r\n",
    "    \"\"\"\r\n",
    "    return np.random.uniform(-1, 1, shape) * np.sqrt(1 / inp)\r\n",
    "\r\n",
    "def xavier_init(inp: int, out: int, shape: List[int]) -> np.ndarray:\r\n",
    "    \"\"\"\r\n",
    "    Generates initalized weights of a given shape\r\n",
    "    :param inp: The number of inputs\r\n",
    "    :param out: The number of outputs\r\n",
    "    :param shape: The shape of the weights to be initialized\r\n",
    "    :return: The \"Xavier\"-initalized weights\r\n",
    "    \"\"\"\r\n",
    "    return np.random.uniform(-1, 1, shape) * np.sqrt(6 / (inp + out))\r\n",
    "\r\n",
    "def he_init(inp: int, out: int, shape: List[int]) -> np.ndarray:\r\n",
    "    \"\"\"\r\n",
    "    Generates initalized weights of a given shape\r\n",
    "    :param inp: The number of inputs\r\n",
    "    :param out: The number of outputs\r\n",
    "    :param shape: The shape of the weights to be initialized\r\n",
    "    :return: The \"He\"-initalized weights\r\n",
    "    \"\"\"\r\n",
    "    return np.random.uniform(-1, 1, shape) * np.sqrt(2 / inp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Layer Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "class Layer(object):\r\n",
    "    \"\"\"\r\n",
    "    Represents a layer in a deep neural network.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, actf: Callable[[np.ndarray], np.ndarray] = relu, actf_g: Callable[[np.ndarray], np.ndarray] = relu_g,\r\n",
    "                 initializer: Callable[[int, int, List[int]], np.ndarray] = he_init, inp_shape: Optional[List[int]] = None):\r\n",
    "        \"\"\"\r\n",
    "        Create a new layer.\r\n",
    "        :param actf: The activation function to use\r\n",
    "        :param actf_g: The activation function's derivation\r\n",
    "        :param initializer: The weight initializer to use\r\n",
    "        :param inp_shape: The shape of the layer's input\r\n",
    "        \"\"\"\r\n",
    "        self.actf: Callable[[np.ndarray], np.ndarray] = actf\r\n",
    "        self.actf_g: Callable[[np.ndarray], np.ndarray] = actf_g\r\n",
    "        self.initializer: Callable[[int, int, List[int]], np.ndarray] = initializer\r\n",
    "        self.weights: Optional[np.ndarray] = None\r\n",
    "        self.inp_shape: Optional[List[int]] = inp_shape\r\n",
    "\r\n",
    "    def initialize(self) -> None:\r\n",
    "        \"\"\"Initializes the weights\"\"\"\r\n",
    "        raise NotImplementedError()\r\n",
    "\r\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        raise NotImplementedError()\r\n",
    "    \r\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform backward propagation + return the results.\"\"\"\r\n",
    "        raise NotImplementedError()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "class FCLayer(Layer):\r\n",
    "    \"\"\"\r\n",
    "    Represents a fully-connected layer in a deep neural network.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, neurons: int, actf: Callable[[np.ndarray], np.ndarray] = relu, actf_g: Callable[[np.ndarray], np.ndarray] = relu_g,\r\n",
    "                 initializer: Callable[[int, int, List[int]], np.ndarray] = he_init, inp_shape: Optional[List[int]] = None):\r\n",
    "        \"\"\"\r\n",
    "        Create a new fully-connected layer.\r\n",
    "        :param neurons: The number of neurons in the layer\r\n",
    "        :param actf: The activation function to use\r\n",
    "        :param actf_g: The activation function's derivation\r\n",
    "        :param initializer: The weight initializer to use\r\n",
    "        :param inp_shape: The shape of the layer's input - format (#neurons)\r\n",
    "        \"\"\"\r\n",
    "        self.neurons: int = neurons\r\n",
    "        super().__init__(actf, actf_g, initializer, inp_shape)\r\n",
    "\r\n",
    "    def initialize(self) -> None:\r\n",
    "        \"\"\"Initializes the weights\"\"\"\r\n",
    "        if self.inp_shape is None:\r\n",
    "            raise TypeError()\r\n",
    "        self.weights = self.initializer(self.inp_shape[0] + 1, self.neurons, [ self.inp_shape[0] + 1, self.neurons, ])\r\n",
    "\r\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        return self.actf(self._forward(inp))\r\n",
    "\r\n",
    "    def _forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation - without applying the activation function.\"\"\"\r\n",
    "        assert len(inp.shape) == 2\r\n",
    "        assert inp.shape[0] == self.neurons\r\n",
    "        return self.weights.T @ np.vstack((inp, np.ones(inp.shape[1])))\r\n",
    "    \r\n",
    "    def backward(self, grad: np.ndarray, val: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"\r\n",
    "        Perform backward propagation + return the results.\r\n",
    "        :param grad: The gradient of the next layer\r\n",
    "        :param val: This layer's value (= before application of activation function)\r\n",
    "        :return: This layer's gradient\r\n",
    "        \"\"\"\r\n",
    "        assert len(grad.shape) == 2\r\n",
    "        assert grad.shape == val.shape\r\n",
    "        assert grad.shape[0] == self.neurons\r\n",
    "        return self.weights[:-1, :] @ (grad * self.actf_g(val))\r\n",
    "\r\n",
    "    def _backward(self, grad: np.ndarray, val: np.ndarray, p_val: np.ndarray, lam: float) -> np.ndarray:\r\n",
    "        \"\"\"\r\n",
    "        Perform backward propagation - but return the weight's gradients.\r\n",
    "        :param grad: The gradient of the next layer\r\n",
    "        :param val: This layer's value (= before application of activation function)\r\n",
    "        :param p_val: The previous layer's value (= before application of activation function)\r\n",
    "        :param lam: The regularization parameter lambda - for weight regularization\r\n",
    "        :return: This layer's gradient\r\n",
    "        \"\"\"\r\n",
    "        assert len(grad.shape) == 2\r\n",
    "        assert grad.shape == val.shape\r\n",
    "        assert grad.shape[0] == self.neurons\r\n",
    "        assert p_val.shape[1] == val.shape[1]\r\n",
    "        assert p_val.shape[0] == self.inp_shape[0]\r\n",
    "        return np.vstack((p_val, np.ones(p_val.shape[1]))) @ (grad * self.actf_g(val)).T + (lam/val.shape[1]) * self.weights\r\n",
    "        \r\n",
    "    def __setattr__(self, name: str, value: Any) -> None:\r\n",
    "        if name == 'inp_shape' and len(value) != 1:\r\n",
    "            raise TypeError()\r\n",
    "        elif name in ('neurons', 'inp_shape'):\r\n",
    "            self.__dict__[name] = value\r\n",
    "            if 'neurons' in self.__dict__.keys() and 'inp_shape' in self.__dict__.keys():\r\n",
    "                self.initialize()\r\n",
    "            return\r\n",
    "        super().__setattr__(name, value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "class ConvLayer(Layer):\r\n",
    "    \"\"\"\r\n",
    "    Represents a convolutional layer in a deep neural network.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, shape: List[int], stride: int, actf: Callable[[np.ndarray], np.ndarray] = relu, \r\n",
    "                 actf_g: Callable[[np.ndarray], np.ndarray] = relu_g,\r\n",
    "                 initializer: Callable[[int, int, List[int]], np.ndarray] = he_init, inp_shape: Optional[List[int]] = None):\r\n",
    "        \"\"\"\r\n",
    "        Create a new convolutional layer.\r\n",
    "        :param shape: The shape of the layer's filters - format (#filters x height x width)\r\n",
    "        :param stride: The filters' stride\r\n",
    "        :param actf: The activation function to use\r\n",
    "        :param actf_g: The activation function's derivation\r\n",
    "        :param initializer: The weight initializer to use\r\n",
    "        :param inp_shape: The shape of the layer's input - format (height x width x #channels)\r\n",
    "        \"\"\"\r\n",
    "        self.shape: List[int] = shape\r\n",
    "        self.stride: int = stride\r\n",
    "        super().__init__(actf, actf_g, initializer, inp_shape)\r\n",
    "\r\n",
    "    def initialize(self) -> None:\r\n",
    "        \"\"\"Initializes the weights\"\"\"\r\n",
    "        if self.inp_shape is None:\r\n",
    "            raise TypeError()\r\n",
    "        filters_size: int = reduce(lambda x, y: x*y, self.shape)\r\n",
    "        self.weights = self.initializer(filters_size * self.inp_shape[2], filters_size, [ *self.shape, self.inp_shape[2], ])\r\n",
    "    \r\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation + return the results.\"\"\"\r\n",
    "        assert len(inp.shape) == 4\r\n",
    "        assert inp.shape[1:] == tuple(self.inp_shape)\r\n",
    "        strides: np.ndarray = np.lib.stride_tricks.sliding_window_view(inp, (inp.shape[0], *self.weights.shape[1:]))\r\n",
    "        strides = np.moveaxis(np.repeat(strides.reshape(*strides.shape[1:3], inp.shape[0], 1, *strides.shape[5:7], \r\n",
    "                                                        self.weights.shape[-1])[::self.stride, ::self.stride], \r\n",
    "                                        self.weights.shape[0], axis=3) * self.weights, 2, 0)\r\n",
    "        return np.sum(strides.reshape(*strides.shape[:4], -1), axis=4)\r\n",
    "\r\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform backward propagation + return the results.\"\"\"\r\n",
    "        assert len(grad.shape) == 4\r\n",
    "        assert grad.shape == (*self.shape[1:], self.shape[0])\r\n",
    "        return super().backward(grad)\r\n",
    "\r\n",
    "    def __setattr__(self, name: str, value: Any) -> None:\r\n",
    "        if name in ('shape', 'inp_shape',):\r\n",
    "            if len(value) != 3:\r\n",
    "                raise TypeError()\r\n",
    "            else:\r\n",
    "                self.__dict__[name] = value\r\n",
    "                if 'shape' in self.__dict__.keys() and 'inp_shape' in self.__dict__.keys():\r\n",
    "                    self.initialize()\r\n",
    "                return\r\n",
    "        super().__setattr__(name, value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class Network(object):\r\n",
    "    \"\"\"\r\n",
    "    Represents a deep neural network.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, initializer: Callable[[List[int]], List[np.ndarray]], layers: List[Layer] = []):\r\n",
    "        \"\"\"\r\n",
    "        Create a new deep neural network.\r\n",
    "        :param initializer: The weight initializer to use\r\n",
    "        :param layers: The network's layers\r\n",
    "        \"\"\"\r\n",
    "        self.initializer: Callable[[List[int]], List[np.ndarray]] = initializer\r\n",
    "        self.layers: List[Layer] = layers\r\n",
    "        self.__initialize_weights()\r\n",
    "\r\n",
    "    def __initialize_weights(self) -> None:\r\n",
    "        \"\"\"Initializes the network's weights\"\"\"\r\n",
    "        self.weights: List[np.ndarray] = self.initializer([ l.neurons for l in self.layers ])\r\n",
    "\r\n",
    "    def append(self, layer: Layer) -> None:\r\n",
    "        \"\"\"Appends a new layer to the network\"\"\"\r\n",
    "        self.layers.append(layer)\r\n",
    "        self.__initialize_weights()\r\n",
    "\r\n",
    "    def insert(self, idx: int, layer: Layer) -> None:\r\n",
    "        \"\"\"Inserts the given layer at the given index\"\"\"\r\n",
    "        self.layers.insert(idx, layer)\r\n",
    "        self.__initialize_weights()\r\n",
    "\r\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\r\n",
    "        \"\"\"Perform forward propagation using the given input\"\"\"\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __add__(self, other: Any) -> Any:\r\n",
    "        if other.__class__ != Layer:\r\n",
    "            raise ArithmeticError()\r\n",
    "        return Network(self.initializer, [ *self.layers.copy(), other, ])\r\n",
    "\r\n",
    "    def __iadd__(self, other: Any) -> None:\r\n",
    "        if other.__class__ != Layer:\r\n",
    "            raise ArithmeticError()\r\n",
    "        self.layers.append(other)\r\n",
    "        self.__initialize_weights()\r\n",
    "\r\n",
    "    def __len__(self) -> int:\r\n",
    "        return len(self.layers)\r\n",
    "\r\n",
    "    def __getitem__(self, key: int) -> Layer:\r\n",
    "        return self.layers[key]\r\n",
    "\r\n",
    "    def __setitem__(self, key: int, value: Layer) -> None:\r\n",
    "        self.layers[key] = value\r\n",
    "        self.__initialize_weights()\r\n",
    "    \r\n",
    "    def __delitem__(self, key: int) -> None:\r\n",
    "        del self.layers[key]\r\n",
    "        self.__initialize_weights()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit"
  },
  "interpreter": {
   "hash": "2f4b728787ec198e2dbbb031677efb56b168098f9046d913c03fa1c4e7b35f03"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}